{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSpark : Illuminating Insights for Global Electronics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "customers = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\Customers.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Handling Missing Values\n",
    "customers['City'].fillna('Unknown', inplace=True)\n",
    "customers['State'].fillna('Unknown', inplace=True)\n",
    "customers['State Code'].fillna('Unknown', inplace=True)  \n",
    "customers['Zip Code'].fillna(0, inplace=True)\n",
    "customers['Country'].fillna('Unknown', inplace=True)\n",
    "customers['Continent'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Correcting Data Types\n",
    "customers['Birthday'] = pd.to_datetime(customers['Birthday'], errors='coerce')\n",
    "\n",
    "# Removing Duplicates (excluding primary key and unique key)\n",
    "customers.drop_duplicates(subset=[col for col in customers.columns if col not in ['CustomerKey']], inplace=True)\n",
    "\n",
    "# Renaming Columns for Consistency\n",
    "customers.rename(columns={'CustomerKey': 'Customer_ID', 'Zip Code': 'Zip_Code', 'State Code': 'State_Code'}, inplace=True)\n",
    "\n",
    "# Handling Categorical Data\n",
    "customers['Gender'] = customers['Gender'].astype('category')\n",
    "\n",
    "# Calculate Age\n",
    "customers['Age'] = (pd.Timestamp.now() - customers['Birthday']).dt.total_seconds() / (60*60*24*365.25)\n",
    "customers['Age'] = customers['Age'].astype(int)\n",
    "\n",
    "# Save cleaned dataset\n",
    "customers.to_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_customers.csv', index=False)\n",
    "\n",
    "print(\"Customers data cleaning completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "sales = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\Sales.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Define primary and unique key columns\n",
    "primary_keys_sales = ['Order Number', 'Line Item', 'CustomerKey', 'StoreKey', 'ProductKey']\n",
    "\n",
    "# Handling Missing Values\n",
    "sales.dropna(subset=['Order Date'], inplace=True)\n",
    "\n",
    "# Correcting Data Types\n",
    "sales['Order Date'] = pd.to_datetime(sales['Order Date'], errors='coerce')\n",
    "\n",
    "# Handling Outliers (e.g., capping extreme values in Quantity)\n",
    "sales['Quantity'] = np.where(sales['Quantity'] > sales['Quantity'].quantile(0.99), sales['Quantity'].quantile(0.99), sales['Quantity'])\n",
    "\n",
    "# Removing Duplicates\n",
    "sales.drop_duplicates(inplace=True)\n",
    "\n",
    "# High Percentage of Missing Values\n",
    "threshold = 0.5\n",
    "columns_to_drop = [col for col in sales.columns if sales[col].isnull().sum() / len(sales) > threshold and col not in primary_keys_sales]\n",
    "\n",
    "# Drop irrelevant, low variance, redundant, high correlation, and privacy-sensitive columns as per conditions\n",
    "sales.drop(columns=set(columns_to_drop), inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "sales.to_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_sales.csv', index=False)\n",
    "\n",
    "print(\"Sales data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StoreKey    Country                         State  Square Meters  Open Date\n",
      "0         1  Australia  Australian Capital Territory          595.0   1/1/2008\n",
      "1         2  Australia            Northern Territory          665.0  1/12/2008\n",
      "2         3  Australia               South Australia         2000.0   1/7/2012\n",
      "3         4  Australia                      Tasmania         2000.0   1/1/2010\n",
      "4         5  Australia                      Victoria         2000.0  12/9/2015\n",
      "Stores data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "stores = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\Stores.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first few rows to understand the data\n",
    "print(stores.head())\n",
    "\n",
    "# Handling Missing Values\n",
    "\n",
    "stores['Country'].fillna('Unknown', inplace=True)\n",
    "stores['State'].fillna('Unknown', inplace=True)\n",
    "stores['Square Meters'].fillna(0, inplace=True)\n",
    "\n",
    "# Correcting Data Types\n",
    "stores['Open Date'] = pd.to_datetime(stores['Open Date'], errors='coerce')\n",
    "\n",
    "# Removing Duplicates (excluding primary key)\n",
    "stores.drop_duplicates(subset=[col for col in stores.columns if col not in ['StoreKey']], inplace=True)\n",
    "\n",
    "# Handle Inconsistent Data (if any)\n",
    "\n",
    "# Save cleaned dataset\n",
    "stores.to_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_stores.csv', index=False)\n",
    "\n",
    "print(\"Stores data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Unit Cost USD to numeric successfully.\n",
      "Converted Unit Price USD to numeric successfully.\n",
      "Products data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "products = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\Products.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Handling Missing Values\n",
    "products['Color'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Removing Duplicates\n",
    "products.drop_duplicates(inplace=True)\n",
    "\n",
    "# Remove currency symbols before converting to numeric\n",
    "for column in ['Unit Cost USD', 'Unit Price USD']:\n",
    "    products[column] = products[column].str.replace('$', '').str.replace(',', '')\n",
    "\n",
    "# Correcting Data Types (e.g., converting cost and price to numeric)\n",
    "for column in ['Unit Cost USD', 'Unit Price USD']:\n",
    "    try:\n",
    "        products[column] = pd.to_numeric(products[column], errors='coerce')\n",
    "        print(f\"Converted {column} to numeric successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {column} to numeric: {e}\")\n",
    "\n",
    "# Additional step to handle missing values in cost and price columns\n",
    "for column in ['Unit Cost USD', 'Unit Price USD']:\n",
    "    if products[column].isnull().any():\n",
    "        products[column].fillna(products[column].median(), inplace=True)\n",
    "        print(f\"Filled missing values in {column} with the median.\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "products.to_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_products.csv', index=False)\n",
    "\n",
    "print(\"Products data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exchange rates data cleaning completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "exchange_rates = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\Exchange_Rates.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Correcting Data Types\n",
    "exchange_rates['Date'] = pd.to_datetime(exchange_rates['Date'], errors='coerce')\n",
    "\n",
    "# Removing Duplicates\n",
    "exchange_rates.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save cleaned dataset\n",
    "exchange_rates.to_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_exchange_rates.csv', index=False)\n",
    "\n",
    "print(\"Exchange rates data cleaning completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merging completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned datasets\n",
    "customers = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_customers.csv', encoding='ISO-8859-1')\n",
    "sales = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_sales.csv', encoding='ISO-8859-1')\n",
    "stores = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_stores.csv', encoding='ISO-8859-1')\n",
    "products = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_products.csv', encoding='ISO-8859-1')\n",
    "exchange_rates = pd.read_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_exchange_rates.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Merge sales with customers\n",
    "sales_customers = pd.merge(sales, customers, how='left', left_on='CustomerKey', right_on='Customer_ID')\n",
    "\n",
    "# Merge sales_customers with stores\n",
    "sales_customers_stores = pd.merge(sales_customers, stores, how='left', left_on='StoreKey', right_on='StoreKey')\n",
    "\n",
    "# Merge sales_customers_stores with products\n",
    "sales_customers_stores_products = pd.merge(sales_customers_stores, products, how='left', left_on='ProductKey', right_on='ProductKey')\n",
    "\n",
    "# Optionally, merge with exchange rates if needed (e.g., if there are columns that need exchange rates)\n",
    "# Assuming you might want to join on a common date column, which isn't directly mentioned in the given datasets\n",
    "# sales_customers_stores_products = pd.merge(sales_customers_stores_products, exchange_rates, how='left', left_on='Date', right_on='Date')\n",
    "\n",
    "# Save the merged dataset\n",
    "sales_customers_stores_products.to_csv('D:\\\\Guvi\\\\projects\\\\Data Spark\\\\merged_data.csv', index=False)\n",
    "\n",
    "print(\"Data merging completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets successfully stored in the SQLite database.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to the cleaned datasets\n",
    "customers_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_customers.csv'\n",
    "sales_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_sales.csv'\n",
    "stores_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_stores.csv'\n",
    "products_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_products.csv'\n",
    "exchange_rates_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\cleaned_exchange_rates.csv'\n",
    "merged_data_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\merged_data.csv'\n",
    "\n",
    "# Path to the SQLite database\n",
    "db_path = 'D:\\\\Guvi\\\\projects\\\\Data Spark\\\\Data_Spark.db'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load each dataset\n",
    "customers = pd.read_csv(customers_path)\n",
    "sales = pd.read_csv(sales_path)\n",
    "stores = pd.read_csv(stores_path)\n",
    "products = pd.read_csv(products_path)\n",
    "exchange_rates = pd.read_csv(exchange_rates_path)\n",
    "merged_data = pd.read_csv(merged_data_path)\n",
    "\n",
    "# Insert each DataFrame into the SQLite database\n",
    "customers.to_sql('Customers', conn, if_exists='replace', index=False)\n",
    "sales.to_sql('Sales', conn, if_exists='replace', index=False)\n",
    "stores.to_sql('Stores', conn, if_exists='replace', index=False)\n",
    "products.to_sql('Products', conn, if_exists='replace', index=False)\n",
    "exchange_rates.to_sql('ExchangeRates', conn, if_exists='replace', index=False)\n",
    "merged_data.to_sql('MergedData', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "print(\"All datasets successfully stored in the SQLite database.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
